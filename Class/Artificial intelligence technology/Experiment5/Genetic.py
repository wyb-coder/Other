"""
å®éªŒäº”ï¼šåŸºäºé—ä¼ ç®—æ³•çš„ç¥ç»ç½‘ç»œè¶…å‚æ•°ä¼˜åŒ–
GA-HPO: Genetic Algorithm for Hyperparameter Optimization

æœ¬ä»£ç åŸºäºå®éªŒå››çš„ ResNet å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œä½¿ç”¨é—ä¼ ç®—æ³•è‡ªåŠ¨æœç´¢æœ€ä¼˜è¶…å‚æ•°ç»„åˆã€‚

è¿è¡Œæ–¹å¼:
    python Genetic.py

ä½œè€…: è½¯ä»¶ä¸“ä¸š25 - 2025354100103
æ—¥æœŸ: 2025å¹´12æœˆ
"""

# ==================================================================================
#                                   Import Libraries
# ==================================================================================
import numpy as np
import pandas as pd
import torch
import os
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm
import random
import torch.nn.functional as F
from torch.autograd import Variable
from sklearn.model_selection import StratifiedKFold
import copy
import json
from datetime import datetime
import matplotlib.pyplot as plt

# ==================================================================================
#                                   Config (é…ç½®æ¨¡å—)
# ==================================================================================
# -------------------- GPU é…ç½® --------------------
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # è®¾ç½®ä½¿ç”¨çš„ GPU ç¼–å·ï¼Œå¤šå¡ç”¨ "0,1,2"

# -------------------- æ•°æ®é›†é…ç½® --------------------
DATASET_DIR = "../Data"  # æ•°æ®é›†æ ¹ç›®å½•
NUM_CLASSES = 11         # åˆ†ç±»ç±»åˆ«æ•°

# -------------------- é—ä¼ ç®—æ³•é…ç½® --------------------
GA_CONFIG = {
    'pop_size': 10,            # ç§ç¾¤å¤§å°ï¼ˆæ ¹æ®è®¡ç®—èµ„æºè°ƒæ•´ï¼Œæ¨è 10-20ï¼‰
    'max_generations': 15,      # æœ€å¤§è¿›åŒ–ä»£æ•°ï¼ˆæ¨è 15-30ï¼‰
    'crossover_rate': 0.8,      # äº¤å‰æ¦‚ç‡
    'mutation_rate': 0.15,      # å˜å¼‚æ¦‚ç‡
    'elite_size': 2,            # ç²¾è‹±ä¿ç•™æ•°é‡
    'eta_c': 20,                # SBX äº¤å‰åˆ†å¸ƒæŒ‡æ•°
    'eta_m': 20,                # å¤šé¡¹å¼å˜å¼‚åˆ†å¸ƒæŒ‡æ•°
}

# -------------------- å¿«é€Ÿè¯„ä¼°é…ç½® --------------------
EVAL_EPOCHS = 5              # æ¯ä¸ªä¸ªä½“å¿«é€Ÿè¯„ä¼°çš„è®­ç»ƒè½®æ•°
EVAL_PATIENCE = 3            # å¿«é€Ÿè¯„ä¼°æ—¶çš„æ—©åœè€å¿ƒå€¼

# -------------------- å®Œæ•´è®­ç»ƒé…ç½®ï¼ˆç”¨äºæœ€ç»ˆæœ€ä¼˜è¶…å‚æ•°ï¼‰ --------------------
FULL_TRAIN_EPOCHS = 300      # å®Œæ•´è®­ç»ƒè½®æ•°
FULL_TRAIN_PATIENCE = 20     # å®Œæ•´è®­ç»ƒæ—©åœè€å¿ƒå€¼

# -------------------- éšæœºç§å­ --------------------
RANDOM_SEED = 5201314        # éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°

# -------------------- Focal Loss ç±»åˆ«æƒé‡ï¼ˆæ¥è‡ªå®éªŒå››ï¼‰ --------------------
FOCAL_ALPHA = torch.Tensor([1, 2.3, 0.66, 1, 1.1, 0.75, 2.3, 3.5, 1.1, 0.66, 1.4]).view(-1, 1)

# -------------------- å®éªŒåç§° --------------------
EXP_NAME = "GA_HPO_Experiment5"

# ==================================================================================
#                               éšæœºç§å­è®¾ç½®
# ==================================================================================
def set_seed(seed=RANDOM_SEED):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(5201314)

# ==================================================================================
#                               Image Transforms
# ==================================================================================
test_tfm = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

train_tfm = transforms.Compose([
    transforms.RandomResizedCrop(128, scale=(0.7, 1.0)),
    transforms.RandomHorizontalFlip(0.5),
    transforms.RandomVerticalFlip(0.5),
    transforms.RandomRotation(180),
    transforms.RandomAffine(30),
    transforms.RandomGrayscale(0.2),
    transforms.ToTensor(),
    transforms.RandomErasing(0.2)
])

# ==================================================================================
#                                   Dataset
# ==================================================================================
class FoodDataset(Dataset):
    """é£Ÿç‰©å›¾åƒæ•°æ®é›†ç±»"""
    
    def __init__(self, path=None, tfm=test_tfm, files=None):
        super(FoodDataset).__init__()
        self.path = path
        if path:
            self.files = sorted([os.path.join(path, x) for x in os.listdir(path) if x.endswith(".jpg")])
        else:
            self.files = files
        self.transform = tfm
        print(f'Dataset size: {len(self.files)} images')

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        fname = self.files[idx]
        im = Image.open(fname)
        im = self.transform(im)
        try:
            label = int(fname.split(os.sep)[-1].split("_")[0])
        except:
            label = -1
        return im, label


# ==================================================================================
#                               Model Structure (æ¥è‡ªå®éªŒå››)
# ==================================================================================
class Residual_Block(nn.Module):
    """æ®‹å·®å—"""
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(out_channels),
        )
        self.relu = nn.ReLU(inplace=True)
        self.downsample = None
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels),
            )

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.conv2(out)
        if self.downsample:
            residual = self.downsample(x)
        out += residual
        return self.relu(out)


class Classifier(nn.Module):
    """åˆ†ç±»å™¨æ¨¡å‹ (å¯é…ç½®å‚æ•°)"""
    def __init__(self, block, num_layers, num_classes=11, dropout1=0.4, dropout2=0.2):
        super(Classifier, self).__init__()
        self.preConv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True)
        )
        self.layer0 = self.makeResidualBlocks(block, 32, 64, num_layers[0], stride=2)
        self.layer1 = self.makeResidualBlocks(block, 64, 128, num_layers[1], stride=2)
        self.layer2 = self.makeResidualBlocks(block, 128, 256, num_layers[2], stride=2)
        self.layer3 = self.makeResidualBlocks(block, 256, 512, num_layers[3], stride=2)

        self.fc = nn.Sequential(
            nn.Dropout(dropout1),
            nn.Linear(512 * 4 * 4, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout2),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        out = self.preConv(x)
        out = self.layer0(out)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.fc(out.view(out.size(0), -1))
        return out

    def makeResidualBlocks(self, block, in_channels, out_channels, num_layer, stride=1):
        layers = [block(in_channels, out_channels, stride)]
        for i in range(1, num_layer):
            layers.append(block(out_channels, out_channels))
        return nn.Sequential(*layers)


class FocalLoss(nn.Module):
    """Focal Loss æŸå¤±å‡½æ•°"""
    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):
        super().__init__()
        if alpha is None:
            self.alpha = Variable(torch.ones(class_num, 1))
        else:
            if isinstance(alpha, Variable):
                self.alpha = alpha
            else:
                self.alpha = Variable(alpha)
        self.gamma = gamma
        self.class_num = class_num
        self.size_average = size_average

    def forward(self, inputs, targets):
        N = inputs.size(0)
        C = inputs.size(1)
        P = F.softmax(inputs, dim=1)

        class_mask = inputs.data.new(N, C).fill_(0)
        class_mask = Variable(class_mask)
        ids = targets.view(-1, 1)
        class_mask.scatter_(1, ids.data, 1.)

        if inputs.is_cuda and not self.alpha.is_cuda:
            self.alpha = self.alpha.cuda()
        alpha = self.alpha[ids.data.view(-1)]
        probs = (P * class_mask).sum(1).view(-1, 1)
        log_p = probs.log()
        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p
        
        if self.size_average:
            loss = batch_loss.mean()
        else:
            loss = batch_loss.sum()
        return loss


# ==================================================================================
#                               é—ä¼ ç®—æ³•æ ¸å¿ƒå®ç°
# ==================================================================================
class Individual:
    """
    ä¸ªä½“ç±»ï¼šè¡¨ç¤ºä¸€ç»„è¶…å‚æ•°ç»„åˆ
    
    æŸ“è‰²ä½“ç»“æ„ (8ä¸ªåŸºå› ä½):
    [0] log10(lr)      : å­¦ä¹ ç‡çš„å¯¹æ•° [-5, -2]
    [1] batch_size_idx : æ‰¹å¤§å°ç´¢å¼• {0,1,2,3} -> {32,64,128,256}
    [2] num_layer_0    : ç¬¬0å±‚æ®‹å·®å—æ•°é‡ [1,4]
    [3] num_layer_1    : ç¬¬1å±‚æ®‹å·®å—æ•°é‡ [1,4]
    [4] num_layer_2    : ç¬¬2å±‚æ®‹å·®å—æ•°é‡ [1,4]
    [5] num_layer_3    : ç¬¬3å±‚æ®‹å·®å—æ•°é‡ [1,4]
    [6] dropout1       : ç¬¬ä¸€ä¸ªDropoutç‡ [0.1, 0.5]
    [7] focal_gamma    : Focal Loss gammaå‚æ•° [0.5, 5.0]
    """
    
    # è¶…å‚æ•°æœç´¢ç©ºé—´å®šä¹‰ - é’ˆå¯¹ RTX 4090 (24GB) æ‰©å±• batch_size
    BATCH_SIZES = [64, 128, 256, 512]
    BOUNDS = {
        'log_lr': (-5, -2),
        'batch_idx': (0, 3),
        'num_layer': (1, 4),
        'dropout': (0.1, 0.5),
        'focal_gamma': (0.5, 5.0)
    }
    
    def __init__(self, chromosome=None):
        if chromosome is None:
            self.chromosome = self._random_init()
        else:
            self.chromosome = list(chromosome)
        self.fitness = None
        self.val_acc = None
    
    def _random_init(self):
        """éšæœºåˆå§‹åŒ–æŸ“è‰²ä½“"""
        return [
            np.random.uniform(*self.BOUNDS['log_lr']),      # log10(lr)
            np.random.randint(0, 4),                         # batch_size index
            np.random.randint(1, 5),                         # num_layers[0]
            np.random.randint(1, 5),                         # num_layers[1]
            np.random.randint(1, 5),                         # num_layers[2]
            np.random.randint(1, 5),                         # num_layers[3]
            np.random.uniform(*self.BOUNDS['dropout']),      # dropout1
            np.random.uniform(*self.BOUNDS['focal_gamma']),  # focal_gamma
        ]
    
    def decode(self):
        """å°†æŸ“è‰²ä½“è§£ç ä¸ºå®é™…è¶…å‚æ•°å­—å…¸"""
        return {
            'lr': 10 ** self.chromosome[0],
            'batch_size': self.BATCH_SIZES[int(np.clip(self.chromosome[1], 0, 3))],
            'num_layers': [
                int(np.clip(self.chromosome[2], 1, 4)),
                int(np.clip(self.chromosome[3], 1, 4)),
                int(np.clip(self.chromosome[4], 1, 4)),
                int(np.clip(self.chromosome[5], 1, 4)),
            ],
            'dropout1': np.clip(self.chromosome[6], 0.1, 0.5),
            'dropout2': np.clip(self.chromosome[6] * 0.5, 0.05, 0.25),  # dropout2 = dropout1 / 2
            'focal_gamma': np.clip(self.chromosome[7], 0.5, 5.0)
        }
    
    def __str__(self):
        hp = self.decode()
        return (f"lr={hp['lr']:.2e}, bs={hp['batch_size']}, "
                f"layers={hp['num_layers']}, drop={hp['dropout1']:.2f}, "
                f"gamma={hp['focal_gamma']:.2f}, fitness={self.fitness:.4f}" if self.fitness else "æœªè¯„ä¼°")
    
    def copy(self):
        """æ·±æ‹·è´ä¸ªä½“"""
        new_ind = Individual(self.chromosome.copy())
        new_ind.fitness = self.fitness
        new_ind.val_acc = self.val_acc
        return new_ind


class GeneticAlgorithm:
    """
    é—ä¼ ç®—æ³•å¼•æ“
    
    å®ç°äº†:
    - è½®ç›˜èµŒé€‰æ‹© + ç²¾è‹±ä¿ç•™
    - æ¨¡æ‹ŸäºŒè¿›åˆ¶äº¤å‰ (SBX)
    - å¤šé¡¹å¼å˜å¼‚
    """
    
    def __init__(self, pop_size=10, max_generations=15, 
                 crossover_rate=0.8, mutation_rate=0.15, 
                 elite_size=2, eta_c=20, eta_m=20):
        """
        åˆå§‹åŒ–é—ä¼ ç®—æ³•
        
        Args:
            pop_size: ç§ç¾¤å¤§å°
            max_generations: æœ€å¤§è¿›åŒ–ä»£æ•°
            crossover_rate: äº¤å‰æ¦‚ç‡
            mutation_rate: å˜å¼‚æ¦‚ç‡
            elite_size: ç²¾è‹±ä¿ç•™æ•°é‡
            eta_c: SBXäº¤å‰åˆ†å¸ƒæŒ‡æ•°
            eta_m: å¤šé¡¹å¼å˜å¼‚åˆ†å¸ƒæŒ‡æ•°
        """
        self.pop_size = pop_size
        self.max_gen = max_generations
        self.pc = crossover_rate
        self.pm = mutation_rate
        self.elite_size = elite_size
        self.eta_c = eta_c
        self.eta_m = eta_m
        
        # è¿›åŒ–å†å²è®°å½•
        self.history = {
            'best_fitness': [],
            'avg_fitness': [],
            'best_individual': [],
            'generation_time': []
        }
    
    def _roulette_selection(self, population, num_select):
        """è½®ç›˜èµŒé€‰æ‹©"""
        fitnesses = np.array([ind.fitness for ind in population])
        # å¤„ç†è´Ÿé€‚åº”åº¦ï¼ˆè™½ç„¶æœ¬å®éªŒä¸­å‡†ç¡®ç‡ä¸ä¼šä¸ºè´Ÿï¼‰
        min_fit = fitnesses.min()
        if min_fit < 0:
            fitnesses = fitnesses - min_fit + 1e-6
        
        # æ·»åŠ å°å¸¸æ•°é¿å…é™¤é›¶
        fitnesses = fitnesses + 1e-10
        probs = fitnesses / fitnesses.sum()
        
        selected_indices = np.random.choice(len(population), size=num_select, p=probs)
        return [population[i].copy() for i in selected_indices]
    
    def _tournament_selection(self, population, num_select, tournament_size=3):
        """é”¦æ ‡èµ›é€‰æ‹©ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
        selected = []
        for _ in range(num_select):
            contestants = random.sample(population, min(tournament_size, len(population)))
            winner = max(contestants, key=lambda x: x.fitness)
            selected.append(winner.copy())
        return selected
    
    def _sbx_crossover(self, parent1, parent2):
        """
        æ¨¡æ‹ŸäºŒè¿›åˆ¶äº¤å‰ (Simulated Binary Crossover)
        """
        child1 = parent1.copy()
        child2 = parent2.copy()
        
        for i in range(len(parent1.chromosome)):
            if random.random() < 0.5:  # æ¯ä¸ªåŸºå› ä½ç‹¬ç«‹å†³å®šæ˜¯å¦äº¤å‰
                u = random.random()
                if u <= 0.5:
                    beta = (2 * u) ** (1.0 / (self.eta_c + 1))
                else:
                    beta = (1.0 / (2 * (1 - u))) ** (1.0 / (self.eta_c + 1))
                
                p1 = parent1.chromosome[i]
                p2 = parent2.chromosome[i]
                
                child1.chromosome[i] = 0.5 * ((1 + beta) * p1 + (1 - beta) * p2)
                child2.chromosome[i] = 0.5 * ((1 - beta) * p1 + (1 + beta) * p2)
        
        child1.fitness = None
        child2.fitness = None
        return child1, child2
    
    def _polynomial_mutation(self, individual):
        """
        å¤šé¡¹å¼å˜å¼‚ (Polynomial Mutation)
        """
        bounds = [
            Individual.BOUNDS['log_lr'],      # 0: log_lr
            (0, 3),                            # 1: batch_idx
            (1, 4),                            # 2-5: num_layers
            (1, 4),
            (1, 4),
            (1, 4),
            Individual.BOUNDS['dropout'],     # 6: dropout
            Individual.BOUNDS['focal_gamma'], # 7: focal_gamma
        ]
        
        for i in range(len(individual.chromosome)):
            if random.random() < self.pm:
                x = individual.chromosome[i]
                xl, xu = bounds[i]
                
                delta = min(x - xl, xu - x) / (xu - xl)
                u = random.random()
                
                if u < 0.5:
                    delta_q = (2 * u + (1 - 2 * u) * (1 - delta) ** (self.eta_m + 1)) ** (1.0 / (self.eta_m + 1)) - 1
                else:
                    delta_q = 1 - (2 * (1 - u) + 2 * (u - 0.5) * (1 - delta) ** (self.eta_m + 1)) ** (1.0 / (self.eta_m + 1))
                
                x_new = x + delta_q * (xu - xl)
                individual.chromosome[i] = np.clip(x_new, xl, xu)
                
                # å¯¹äºç¦»æ•£å‚æ•°ï¼Œè¿›è¡Œå–æ•´
                if i in [1, 2, 3, 4, 5]:
                    individual.chromosome[i] = int(round(individual.chromosome[i]))
        
        individual.fitness = None
        return individual
    
    def evolve(self, evaluator, verbose=True):
        """
        ä¸»è¿›åŒ–å¾ªç¯
        
        Args:
            evaluator: è¯„ä¼°å™¨å¯¹è±¡ï¼Œéœ€è¦æœ‰ evaluate(hyperparams) æ–¹æ³•
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            best_individual: æœ€ä¼˜ä¸ªä½“
            history: è¿›åŒ–å†å²
        """
        # åˆå§‹åŒ–ç§ç¾¤
        population = [Individual() for _ in range(self.pop_size)]
        
        print("=" * 70)
        print("       é—ä¼ ç®—æ³•è¶…å‚æ•°ä¼˜åŒ– (GA-HPO) å¼€å§‹")
        print("=" * 70)
        print(f"ç§ç¾¤å¤§å°: {self.pop_size}, æœ€å¤§ä»£æ•°: {self.max_gen}")
        print(f"äº¤å‰ç‡: {self.pc}, å˜å¼‚ç‡: {self.pm}, ç²¾è‹±æ•°: {self.elite_size}")
        print("=" * 70)
        
        for gen in range(self.max_gen):
            gen_start_time = datetime.now()
            
            print(f"\n{'='*20} ç¬¬ {gen+1}/{self.max_gen} ä»£ {'='*20}")
            
            # Step 1: è¯„ä¼°é€‚åº”åº¦
            for idx, ind in enumerate(population):
                if ind.fitness is None:
                    hp = ind.decode()
                    print(f"  è¯„ä¼°ä¸ªä½“ {idx+1}/{len(population)}: lr={hp['lr']:.2e}, "
                          f"bs={hp['batch_size']}, layers={hp['num_layers']}")
                    
                    val_acc = evaluator.evaluate(hp)
                    ind.fitness = val_acc
                    ind.val_acc = val_acc
                    
                    print(f"    -> éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.2f}%")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            fitnesses = [ind.fitness for ind in population]
            best_fitness = max(fitnesses)
            avg_fitness = np.mean(fitnesses)
            best_ind = max(population, key=lambda x: x.fitness)
            
            self.history['best_fitness'].append(best_fitness)
            self.history['avg_fitness'].append(avg_fitness)
            self.history['best_individual'].append(best_ind.decode())
            self.history['generation_time'].append((datetime.now() - gen_start_time).total_seconds())
            
            print(f"\n  [ç»Ÿè®¡] æœ€ä¼˜é€‚åº”åº¦: {best_fitness*100:.2f}% | "
                  f"å¹³å‡é€‚åº”åº¦: {avg_fitness*100:.2f}%")
            print(f"  [æœ€ä¼˜] {best_ind}")
            
            # å¦‚æœæ˜¯æœ€åä¸€ä»£ï¼Œç›´æ¥è¿”å›
            if gen == self.max_gen - 1:
                break
            
            # Step 2: ç²¾è‹±ä¿ç•™
            population.sort(key=lambda x: x.fitness, reverse=True)
            elites = [population[i].copy() for i in range(self.elite_size)]
            
            # Step 3: é€‰æ‹©
            num_offspring = self.pop_size - self.elite_size
            selected = self._roulette_selection(population, num_offspring)
            
            # Step 4: äº¤å‰
            offspring = []
            for i in range(0, len(selected) - 1, 2):
                if random.random() < self.pc:
                    child1, child2 = self._sbx_crossover(selected[i], selected[i+1])
                else:
                    child1, child2 = selected[i].copy(), selected[i+1].copy()
                offspring.extend([child1, child2])
            
            # ç¡®ä¿offspringæ•°é‡æ­£ç¡®
            while len(offspring) < num_offspring:
                offspring.append(selected[0].copy())
            offspring = offspring[:num_offspring]
            
            # Step 5: å˜å¼‚
            offspring = [self._polynomial_mutation(ind) for ind in offspring]
            
            # Step 6: å½¢æˆæ–°ç§ç¾¤
            population = elites + offspring
        
        # è¿”å›æœ€ä¼˜ä¸ªä½“
        best_individual = max(population, key=lambda x: x.fitness)
        
        print("\n" + "=" * 70)
        print("       é—ä¼ ç®—æ³•ä¼˜åŒ–å®Œæˆ!")
        print("=" * 70)
        print(f"æœ€ä¼˜è¶…å‚æ•°ç»„åˆ:")
        best_hp = best_individual.decode()
        for k, v in best_hp.items():
            print(f"  {k}: {v}")
        print(f"æœ€ä¼˜éªŒè¯å‡†ç¡®ç‡: {best_individual.fitness * 100:.2f}%")
        print("=" * 70)
        
        return best_individual, self.history
    
    def plot_evolution(self, save_path=None):
        """ç»˜åˆ¶è¿›åŒ–æ›²çº¿"""
        plt.figure(figsize=(12, 5))
        
        # é€‚åº”åº¦æ›²çº¿
        plt.subplot(1, 2, 1)
        generations = range(1, len(self.history['best_fitness']) + 1)
        plt.plot(generations, [f*100 for f in self.history['best_fitness']], 
                 'b-o', label='æœ€ä¼˜é€‚åº”åº¦', linewidth=2)
        plt.plot(generations, [f*100 for f in self.history['avg_fitness']], 
                 'r--s', label='å¹³å‡é€‚åº”åº¦', linewidth=2)
        plt.xlabel('è¿›åŒ–ä»£æ•°', fontsize=12)
        plt.ylabel('éªŒè¯å‡†ç¡®ç‡ (%)', fontsize=12)
        plt.title('é—ä¼ ç®—æ³•è¿›åŒ–æ›²çº¿', fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # å­¦ä¹ ç‡æ¼”å˜
        plt.subplot(1, 2, 2)
        lrs = [hp['lr'] for hp in self.history['best_individual']]
        plt.semilogy(generations, lrs, 'g-^', linewidth=2)
        plt.xlabel('è¿›åŒ–ä»£æ•°', fontsize=12)
        plt.ylabel('å­¦ä¹ ç‡ (log scale)', fontsize=12)
        plt.title('æœ€ä¼˜ä¸ªä½“å­¦ä¹ ç‡æ¼”å˜', fontsize=14)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"è¿›åŒ–æ›²çº¿å·²ä¿å­˜è‡³: {save_path}")
        
        plt.show()


# ==================================================================================
#                               ç¥ç»ç½‘ç»œè¯„ä¼°å™¨
# ==================================================================================
class NNEvaluator:
    """
    ç¥ç»ç½‘ç»œè¯„ä¼°å™¨
    
    è´Ÿè´£æ ¹æ®è¶…å‚æ•°è®­ç»ƒæ¨¡å‹å¹¶è¿”å›éªŒè¯å‡†ç¡®ç‡
    """
    
    def __init__(self, train_files, val_files, eval_epochs=5, device=None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            train_files: è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            val_files: éªŒè¯é›†æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            eval_epochs: å¿«é€Ÿè¯„ä¼°æ—¶çš„è®­ç»ƒè½®æ•°
            device: è®¡ç®—è®¾å¤‡
        """
        self.train_files = train_files
        self.val_files = val_files
        self.eval_epochs = eval_epochs
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.eval_count = 0
        
        print(f"è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(train_files)} å¼ å›¾åƒ")
        print(f"  éªŒè¯é›†: {len(val_files)} å¼ å›¾åƒ")
        print(f"  è¯„ä¼°è½®æ•°: {eval_epochs}")
        print(f"  è®¾å¤‡: {self.device}")
    
    def evaluate(self, hyperparams):
        """
        è¯„ä¼°è¶…å‚æ•°ç»„åˆ
        
        Args:
            hyperparams: è¶…å‚æ•°å­—å…¸
        
        Returns:
            éªŒè¯é›†å‡†ç¡®ç‡ (0-1)
        """
        self.eval_count += 1
        
        try:
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - num_workers=2 é¿å…æ–‡ä»¶æè¿°ç¬¦è€—å°½
            train_set = FoodDataset(tfm=train_tfm, files=self.train_files)
            train_loader = DataLoader(
                train_set, 
                batch_size=hyperparams['batch_size'],
                shuffle=True, 
                num_workers=2,  # å‡å°‘ worker æ•°é¿å… "Too many open files"
                pin_memory=True
            )
            
            val_set = FoodDataset(tfm=test_tfm, files=self.val_files)
            val_loader = DataLoader(
                val_set, 
                batch_size=hyperparams['batch_size'],
                shuffle=False, 
                num_workers=2,
                pin_memory=True
            )
            
            # æ„å»ºæ¨¡å‹
            model = Classifier(
                block=Residual_Block,
                num_layers=hyperparams['num_layers'],
                num_classes=NUM_CLASSES,
                dropout1=hyperparams['dropout1'],
                dropout2=hyperparams['dropout2']
            ).to(self.device)
            
            # æŸå¤±å‡½æ•° - ä½¿ç”¨é…ç½®ä¸­çš„ç±»åˆ«æƒé‡
            criterion = FocalLoss(
                class_num=NUM_CLASSES,
                alpha=FOCAL_ALPHA,
                gamma=hyperparams['focal_gamma']
            )
            
            # ä¼˜åŒ–å™¨ - ä¸å®éªŒå››ä¸€è‡´
            optimizer = torch.optim.Adam(
                model.parameters(),
                lr=hyperparams['lr'],
                weight_decay=1e-5
            )
            
            # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä¸å®éªŒå››ä¸€è‡´
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer, T_0=16, T_mult=1
            )
            
            # å¿«é€Ÿè®­ç»ƒ - å¸¦è¿›åº¦æ¡
            total_batches = len(train_loader) * self.eval_epochs
            with tqdm(total=total_batches, desc="    è®­ç»ƒä¸­", leave=False, 
                      ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:
                for epoch in range(self.eval_epochs):
                    model.train()
                    for imgs, labels in train_loader:
                        # non_blocking=True å®ç°å¼‚æ­¥ä¼ è¾“
                        imgs = imgs.to(self.device, non_blocking=True)
                        labels = labels.to(self.device, non_blocking=True)
                        
                        optimizer.zero_grad()
                        outputs = model(imgs)
                        loss = criterion(outputs, labels)
                        loss.backward()
                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
                        optimizer.step()
                        pbar.update(1)
                    
                    scheduler.step()  # æ¯ä¸ªepochåæ›´æ–°å­¦ä¹ ç‡
            
            # éªŒè¯è¯„ä¼° - å¸¦è¿›åº¦æ¡
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for imgs, labels in tqdm(val_loader, desc="    éªŒè¯ä¸­", leave=False, 
                                          ncols=80, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'):
                    imgs = imgs.to(self.device, non_blocking=True)
                    labels = labels.to(self.device, non_blocking=True)
                    
                    outputs = model(imgs)
                    _, predicted = outputs.max(1)
                    total += labels.size(0)
                    correct += predicted.eq(labels).sum().item()
            
            val_acc = correct / total
            
            # æ¸…ç†èµ„æº - é‡Šæ”¾ DataLoader å’Œæ¨¡å‹é¿å…æ–‡ä»¶æè¿°ç¬¦æ³„æ¼
            del train_loader, val_loader, train_set, val_set
            del model, optimizer, criterion, scheduler
            torch.cuda.empty_cache()
            
            return val_acc
            
        except Exception as e:
            print(f"    [é”™è¯¯] è¯„ä¼°å¤±è´¥: {e}")
            return 0.0  # è¿”å›æœ€ä½é€‚åº”åº¦


# ==================================================================================
#                               ä¸»å‡½æ•°
# ==================================================================================
def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œé—ä¼ ç®—æ³•è¶…å‚æ•°ä¼˜åŒ–"""
    
    print("\n" + "=" * 70)
    print("   å®éªŒäº”ï¼šåŸºäºé—ä¼ ç®—æ³•çš„ç¥ç»ç½‘ç»œè¶…å‚æ•°ä¼˜åŒ–")
    print("   GA-HPO: Genetic Algorithm for Hyperparameter Optimization")
    print("=" * 70)
    
    # ä½¿ç”¨é¡¶éƒ¨ Config æ¨¡å—ä¸­å®šä¹‰çš„å…¨å±€é…ç½®
    print(f"\n[é…ç½®ä¿¡æ¯]")
    print(f"  GPU: {os.environ.get('CUDA_VISIBLE_DEVICES', 'auto')}")
    print(f"  æ•°æ®é›†ç›®å½•: {DATASET_DIR}")
    print(f"  é—ä¼ ç®—æ³•å‚æ•°: {GA_CONFIG}")
    print(f"  å¿«é€Ÿè¯„ä¼°è½®æ•°: {EVAL_EPOCHS}")
    
    # ==================== å‡†å¤‡æ•°æ® ====================
    print("\n[Step 1] åŠ è½½æ•°æ®é›†...")
    
    train_dir = os.path.join(DATASET_DIR, "training")
    val_dir = os.path.join(DATASET_DIR, "validation")
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(train_dir) or not os.path.exists(val_dir):
        print(f"[é”™è¯¯] æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨!")
        print(f"  è¯·ç¡®ä¿ä»¥ä¸‹ç›®å½•å­˜åœ¨:")
        print(f"  - {train_dir}")
        print(f"  - {val_dir}")
        print("\næ­£åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
        
        # æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
        class MockEvaluator:
            def __init__(self):
                self.eval_count = 0
            
            def evaluate(self, hyperparams):
                """æ¨¡æ‹Ÿè¯„ä¼°å‡½æ•°"""
                self.eval_count += 1
                # åŸºäºè¶…å‚æ•°ç”Ÿæˆæ¨¡æ‹Ÿé€‚åº”åº¦
                lr_score = 1 - abs(np.log10(hyperparams['lr']) + 3.5) / 2.5
                layer_score = sum(hyperparams['num_layers']) / 16
                dropout_score = 1 - abs(hyperparams['dropout1'] - 0.3) / 0.4
                gamma_score = 1 - abs(hyperparams['focal_gamma'] - 2.5) / 4.5
                
                base_score = 0.5 + 0.3 * (lr_score * 0.3 + layer_score * 0.2 + 
                                          dropout_score * 0.25 + gamma_score * 0.25)
                noise = np.random.normal(0, 0.02)
                return np.clip(base_score + noise, 0.3, 0.95)
        
        evaluator = MockEvaluator()
    else:
        # åŠ è½½çœŸå®æ•°æ®
        train_files = [os.path.join(train_dir, x) for x in os.listdir(train_dir) if x.endswith('.jpg')]
        val_files = [os.path.join(val_dir, x) for x in os.listdir(val_dir) if x.endswith('.jpg')]
        
        print(f"  è®­ç»ƒé›†: {len(train_files)} å¼ å›¾åƒ")
        print(f"  éªŒè¯é›†: {len(val_files)} å¼ å›¾åƒ")
        
        evaluator = NNEvaluator(
            train_files=train_files,
            val_files=val_files,
            eval_epochs=EVAL_EPOCHS
        )
    
    # ==================== é—ä¼ ç®—æ³•ä¼˜åŒ– ====================
    print("\n[Step 2] åˆå§‹åŒ–é—ä¼ ç®—æ³•...")
    
    ga = GeneticAlgorithm(**GA_CONFIG)
    
    print("\n[Step 3] å¼€å§‹è¿›åŒ–ä¼˜åŒ–...")
    best_individual, history = ga.evolve(evaluator)
    
    # ==================== ç»“æœè¾“å‡º ====================
    print("\n[Step 4] ä¿å­˜ç»“æœ...")
    
    # ä¿å­˜æœ€ä¼˜è¶…å‚æ•°
    best_hp = best_individual.decode()
    results = {
        'best_hyperparameters': best_hp,
        'best_fitness': best_individual.fitness,
        'evolution_history': {
            'best_fitness': history['best_fitness'],
            'avg_fitness': history['avg_fitness'],
        },
        'ga_config': GA_CONFIG,
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
    def convert_to_serializable(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        return obj
    
    results = convert_to_serializable(results)
    
    results_path = "ga_hpo_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"  ç»“æœå·²ä¿å­˜è‡³: {results_path}")
    
    # ç»˜åˆ¶è¿›åŒ–æ›²çº¿
    print("\n[Step 5] ç»˜åˆ¶è¿›åŒ–æ›²çº¿...")
    try:
        ga.plot_evolution(save_path="evolution_curve.png")
    except Exception as e:
        print(f"  ç»˜å›¾å¤±è´¥ (å¯èƒ½æ˜¯æ— GUIç¯å¢ƒ): {e}")
    
    # ==================== æœ€ç»ˆæŠ¥å‘Š ====================
    print("\n" + "=" * 70)
    print("                    é—ä¼ ç®—æ³•ä¼˜åŒ–å®Œæˆ - æœ€ç»ˆæŠ¥å‘Š")
    print("=" * 70)
    print(f"\nğŸ“Š æœ€ä¼˜è¶…å‚æ•°ç»„åˆ:")
    print(f"   â”œâ”€â”€ å­¦ä¹ ç‡ (lr):        {best_hp['lr']:.6e}")
    print(f"   â”œâ”€â”€ æ‰¹å¤§å° (batch_size): {best_hp['batch_size']}")
    print(f"   â”œâ”€â”€ ç½‘ç»œæ·±åº¦ (layers):   {best_hp['num_layers']}")
    print(f"   â”œâ”€â”€ Dropoutç‡:          {best_hp['dropout1']:.3f}")
    print(f"   â””â”€â”€ Focal Î³:            {best_hp['focal_gamma']:.3f}")
    print(f"\nğŸ¯ æœ€ä¼˜éªŒè¯å‡†ç¡®ç‡: {best_individual.fitness * 100:.2f}%")
    print(f"ğŸ“ˆ è¯„ä¼°æ€»æ¬¡æ•°: {evaluator.eval_count}")
    print(f"â±ï¸  æ€»è€—æ—¶: {sum(history['generation_time']):.1f} ç§’")
    print("=" * 70)
    
    # è¿”å›ç»“æœä¾›å¤–éƒ¨ä½¿ç”¨
    return best_individual, history


# ==================================================================================
#                               ç¨‹åºå…¥å£
# ==================================================================================
if __name__ == "__main__":
    # è®¾ç½® matplotlib ä¸­æ–‡å­—ä½“
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    except:
        pass
    
    # è¿è¡Œä¸»å‡½æ•°
    best, history = main()
