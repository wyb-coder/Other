# ==============================================================================
# Flume Agent Configuration for Traffic Data Collection
# ==============================================================================
# This configuration demonstrates a Lambda Architecture style data pipeline:
#   Source (CSV Spool) -> Channel (File) -> Sink (HDFS + HBase)
#
# Usage:
#   flume-ng agent -n traffic_agent -c conf -f flume-traffic.conf
# ==============================================================================

# Agent name
traffic_agent.sources = csv_source
traffic_agent.channels = file_channel
traffic_agent.sinks = hdfs_sink hbase_sink

# ==============================================================================
# SOURCE: Spooling Directory (watches for new CSV files)
# ==============================================================================
traffic_agent.sources.csv_source.type = spooldir
traffic_agent.sources.csv_source.spoolDir = /data/incoming
traffic_agent.sources.csv_source.fileSuffix = .COMPLETED
traffic_agent.sources.csv_source.fileHeader = true
traffic_agent.sources.csv_source.inputCharset = UTF-8

# Interceptor: Add timestamp to events
traffic_agent.sources.csv_source.interceptors = timestamper
traffic_agent.sources.csv_source.interceptors.timestamper.type = timestamp

# Connect source to channel
traffic_agent.sources.csv_source.channels = file_channel

# ==============================================================================
# CHANNEL: File Channel (durable, survives agent restart)
# ==============================================================================
traffic_agent.channels.file_channel.type = file
traffic_agent.channels.file_channel.checkpointDir = /data/flume/checkpoint
traffic_agent.channels.file_channel.dataDirs = /data/flume/data
traffic_agent.channels.file_channel.capacity = 1000000
traffic_agent.channels.file_channel.transactionCapacity = 10000

# ==============================================================================
# SINK 1: HDFS (Batch Layer - Immutable Logs)
# ==============================================================================
traffic_agent.sinks.hdfs_sink.type = hdfs
traffic_agent.sinks.hdfs_sink.hdfs.path = hdfs://namenode:9000/traffic/raw/%Y-%m-%d/
traffic_agent.sinks.hdfs_sink.hdfs.filePrefix = traffic
traffic_agent.sinks.hdfs_sink.hdfs.fileSuffix = .csv
traffic_agent.sinks.hdfs_sink.hdfs.fileType = DataStream
traffic_agent.sinks.hdfs_sink.hdfs.writeFormat = Text
traffic_agent.sinks.hdfs_sink.hdfs.rollInterval = 3600
traffic_agent.sinks.hdfs_sink.hdfs.rollSize = 134217728
traffic_agent.sinks.hdfs_sink.hdfs.rollCount = 0
traffic_agent.sinks.hdfs_sink.hdfs.batchSize = 1000
traffic_agent.sinks.hdfs_sink.hdfs.useLocalTimeStamp = true

traffic_agent.sinks.hdfs_sink.channel = file_channel

# ==============================================================================
# SINK 2: HBase (Speed Layer - Indexed View)
# ==============================================================================
traffic_agent.sinks.hbase_sink.type = hbase
traffic_agent.sinks.hbase_sink.table = traffic
traffic_agent.sinks.hbase_sink.columnFamily = cf
traffic_agent.sinks.hbase_sink.serializer = org.apache.flume.sink.hbase.RegexHbaseEventSerializer
traffic_agent.sinks.hbase_sink.serializer.regex = ^(.+),(.+),(.+),(.+)$
traffic_agent.sinks.hbase_sink.serializer.colNames = road_seg_id,data_time,volume,speed
traffic_agent.sinks.hbase_sink.zookeeperQuorum = zookeeper:2181
traffic_agent.sinks.hbase_sink.batchSize = 100

traffic_agent.sinks.hbase_sink.channel = file_channel

# ==============================================================================
# Sink Group: Load Balancing (optional, for multiple sinks)
# ==============================================================================
# traffic_agent.sinkgroups = sink_group1
# traffic_agent.sinkgroups.sink_group1.sinks = hdfs_sink hbase_sink
# traffic_agent.sinkgroups.sink_group1.processor.type = load_balance
# traffic_agent.sinkgroups.sink_group1.processor.backoff = true
