======================================================================
Phase 5: 15-Minute Window Aggregation Statistics
======================================================================

Generated at: 2025-12-22 18:16:47

======================================================================
1. Data Overview
======================================================================
  Original records: 93,234
  Aggregated records: 6,717
  Compression ratio: 13.88x
  Window size: 15 minutes

======================================================================
2. Aggregation Results
======================================================================

  Volume (total_volume) per 15-min window:
    Mean: 413.59
    Std:  388.62
    Min:  2
    Max:  1846

  Speed (avg_speed) per 15-min window:
    Mean: 37.94 km/h
    Std:  22.05
    Min:  1.32
    Max:  88.33

  Records per window (should be ~15):
    Mean: 13.9
    Min:  3
    Max:  15

======================================================================
3. Per-Day Breakdown
======================================================================

  Day | Total Volume | Avg Vol/Window | Avg Speed | Windows
  ------------------------------------------------------------
    1 |      438,563 |          457.8 |     37.33 |     958
    2 |      406,246 |          423.6 |     39.05 |     959
    3 |      377,350 |          393.1 |     39.14 |     960
    4 |      387,841 |          404.0 |     37.61 |     960
    5 |      393,209 |          409.6 |     37.46 |     960
    6 |      388,001 |          404.2 |     37.47 |     960
    7 |      386,889 |          403.0 |     37.52 |     960

======================================================================
4. Equivalent Spark Code (for Thesis)
======================================================================

  from pyspark.sql import SparkSession
  from pyspark.sql.functions import col, sum, avg, window
  
  spark = SparkSession.builder.appName("TrafficAggregation").getOrCreate()
  
  df = spark.read.csv("hdfs://namenode:9000/traffic/82_processed.csv", 
                      header=True, inferSchema=True)
  
  result = df.groupBy(
      col("road_seg_id"),
      window(col("data_time"), "15 minutes")
  ).agg(
      sum("volume").alias("total_volume"),
      avg("speed").alias("avg_speed")
  )
  
  result.write.csv("hdfs://namenode:9000/traffic/aggregated_15min")


======================================================================
5. Technical Explanation (for Thesis)
======================================================================

  【窗口聚合原理】
  
  1. 分钟级数据 → 15分钟级数据的转换：
     
     原始: 00:00, 00:01, ..., 00:14 (15条)
             ↓  window("15 minutes")
     聚合: 00:00-00:15 (1条, SUM/AVG)
  
  2. Shuffle 过程：
     - GroupBy 操作触发 Shuffle
     - 相同 (road_seg_id, window) 的数据发送到同一 Reducer
     - Spark 自动优化 Shuffle 分区数
  
  3. 性能优势：
     - Pandas (本地): 适合 < 1GB 数据
     - Spark (分布式): 适合 > 10GB 数据
     - 本实验数据量 (5MB) 两者皆可
